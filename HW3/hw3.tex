\title{Assignment 3: CS 754, Advanced Image Processing}
\author{}
\date{Due: 22nd March before 11:55 pm}

\documentclass[11pt]{article}

\usepackage{amsmath,soul,xcolor}
\usepackage{amssymb}
\usepackage{hyperref}
\usepackage{ulem}
\usepackage[margin=0.5in]{geometry}
\begin{document}
\maketitle

\textbf{Remember the honor code while submitting this (and every other) assignment. All members of the group should work on and \emph{understand} all parts of the assignment. We will adopt a \textbf{zero-tolerance policy} against any violation.}
\\
\\
\textbf{Submission instructions:} You should ideally type out all the answers in Word (with the equation editor) or using Latex. In either case, prepare a pdf file. Create a single zip or rar file containing the report, code and sample outputs and name it as follows: A3-IdNumberOfFirstStudent-IdNumberOfSecondStudent.zip. (If you are doing the assignment alone, the name of the zip file is A3-IdNumber.zip). Upload the file on moodle BEFORE 11:55 pm on 22nd March. No assignments will be accepted after a cutoff deadline of 10 am on 23rd March. Note that only one student per group should upload their work on moodle. Please preserve a copy of all your work until the end of the semester. \emph{If you have difficulties, please do not hesitate to seek help from me.} 

\begin{enumerate}
\item Your task here is to implement the ISTA algorithm for the following three cases:
\begin{enumerate}
\item Consider the image from the homework folder. Add iid Gaussian noise of mean 0 and variance 3 (on a [0,255] scale) to it, using the `randn' function in MATLAB. Thus $\boldsymbol{y} = \boldsymbol{x} + \boldsymbol{\eta}$ where $\boldsymbol{\eta} \sim \mathcal{N}(0,3)$ \textcolor{blue}{(earlier the variance was mistakenly marked as 4)}. You should obtain $\boldsymbol{x}$ from $\boldsymbol{y}$ using the fact that patches from $\boldsymbol{x}$ have a sparse or near-sparse representation in the 2D-DCT basis. 
\item Divide the image shared in the homework folder into patches of size $8 \times 8$. Let $\boldsymbol{x_i}$ be the vectorized version of the $i^{th}$ patch. Consider the measurement $\boldsymbol{y_i} = \boldsymbol{\Phi x_i}$ where $\boldsymbol{\Phi}$ is a $32 \times 64$ matrix with entries drawn iid from $\mathcal{N}(0,1)$. Note that $\boldsymbol{x_i}$ has a near-sparse representation in the 2D-DCT basis $\boldsymbol{U}$ which is computed in MATLAB as `kron(dctmtx(8)',dctmtx(8)')'. In other words, $\boldsymbol{x_i} = \boldsymbol{U \theta_i}$ where $\boldsymbol{\theta_i}$ is a near-sparse vector. Your job is to reconstruct each $\boldsymbol{x_i}$ given $\boldsymbol{y_i}$ and $\boldsymbol{\Phi}$ using ISTA. Then you should reconstruct the image by averaging the overlapping patches. You should choose the $\alpha$ parameter in the ISTA algorithm judiciously. Choose $\lambda = 1$ (for a [0,255] image). Display the reconstructed image in your report. State the RMSE given as $\|X(:)-\hat{X}(:)\|_2/\|X(:)\|_2$ where $\hat{X}$ is the reconstructed image and $X$ is the true image. \textsf{[15 points]}
\end{enumerate}

\item Download the book `Statistical Learning with Sparsity: The Lasso and Generalizations' from \url{https://web.stanford.edu/~hastie/StatLearnSparsity_files/SLS_corrected_1.4.16.pdf}, which is the website of one of the authors. (The book can be officially downloaded from this online source). Your task is to trace through the steps of the proof of Theorem 11.1(b). This theorem essentially derives error bounds on the minimum of the following objective function: $J(\boldsymbol{\beta}) = \dfrac{1}{2N} \|\boldsymbol{y} - \boldsymbol{X \beta}\|^2 + \lambda_N \|\boldsymbol{\beta}\|_1$ where $\lambda_N$ is a regularization parameter, $\boldsymbol{\beta} \in \mathbb{R}^p$ is the unknown sparse signal, $\boldsymbol{y} = \boldsymbol{X \beta} + \boldsymbol{w}$ is a measurement vector with $N$ values, $\boldsymbol{w}$ is a zero-mean i.i.d. Gaussian noise vector whose each element has standard deviation $\sigma$ and $\boldsymbol{X} \in \mathbb{R}^{N \times p}$ is a sensing matrix whose every column is unit normalized. This particular estimator (i.e. minimizer of $J(\boldsymbol{x})$ for $\boldsymbol{x}$) is called the LASSO in the statistics literature. The theorem derives a statistical bound on $\lambda$ also. Your task is split up in the following manner:
\begin{enumerate}
\item Define the restricted eigenvalue condition (the answer's there in the book and you are allowed to read it, but you also need to \emph{understand} it). 
\item Starting from equation 11.20 on page 309 - explain why $G(\hat{v}) \leq G(0)$.
\item Do the algebra to obtain equation 11.21.
\item Do the algebra in more detail to obtain equation 11.22 (state the exact method of application of Holder's inequality - check the wiki article on it, if you want to find out what this inequality states).
\item Derive equation 11.23.
\item Assuming Lemma 11.1 is true and now that you have derived equation 11.23, complete the proof for the final error bound for equation 11.14b.
\item In which part of the proof does the bound $\lambda_N \geq 2 \dfrac{\|\boldsymbol{X}^T \boldsymbol{w}\|_{\infty}}{N}$ show up? Explain.
\item Why is the cone constraint required? You may read the rest of the chapter to find the answer.
\item Read example 11.1 which tells you how to put a tail bound on $\lambda_N$ assuming that the noise vector $\boldsymbol{w}$ is zero-mean Gaussian with standard  deviation $\sigma$. Given this, state the advantages of this theorem over Theorem 3 that we did in class. You may read parts of the rest of the chapter to answer this question. What are the advantages of Theorem 3 over this particular theorem? 
\item Now read Theorem 1.10 till corollary 1.2 and comments on it concerning an estimator called the `Dantzig selector', in the tutorial `Introduction to Compressed Sensing' by Davenport, Duarte, Eldar and Kuttyniok. You can find it here: \url{http://www.ecs.umass.edu/~mduarte/images/IntroCS.pdf} or at \url{https://webee.technion.ac.il/Sites/People/YoninaEldar/files/ddek.pdf}. What is the common thread between the bounds on the `Dantzig selector' and the LASSO? 
\textsf{[2 x 8 + 4 + 4 = 24 points]}
\end{enumerate}

\item In this task, you will you use the well-known package L1\_LS from \url{https://stanford.edu/~boyd/l1_ls/}. This package is often used for compressed sensing solution, but here you will use it for the purpose of tomographic reconstruction. The homework folder contains images of two slices taken from an MR volume of the brain. Create measurements by parallel beam tomographic projections at any 18 randomly angles chosen from a uniform distribution on $[0,\pi)$. Use the MATLAB function `radon' for this purpose. Now perform tomographic reconstruction using the following method: (a) filtered back-projection using the Ram-Lak filter, as implemented in the `iradon' function in MATLAB, (b) independent CS-based reconstruction for each slice by solving an optimization problem of the form $J(\boldsymbol{x}) = \|\boldsymbol{y}-\boldsymbol{Ax}\|^2 + \lambda \|\boldsymbol{x}\|_1$, (c) a coupled CS-based reconstruction that takes into account the similarity of the two slices using the model given in the lectures notes on tomography. For parts (b) and (c), use the aforementioned package from Stanford. For part (c), make sure you use a \emph{different} random set of 18 angles for each of the two slices. The tricky part is careful creation of the forward model matrix $\boldsymbol{A}$ or a function handle representing that matrix, as well as the corresponding adjoint operator $\boldsymbol{A}^T$. Use the 2D-DCT basis for the image representation. Modify the objective function from the lecture notes for the case of three similar slices. Carefully define all terms in the equation but do not re-implement it.
\textcolor{blue}{For ease of implementation, use square images. For this zero-pad the original images to make them square-shaped before getting the radon projections. You can also specify the output size in the iradon function. You may work with uniformly spaced angles instead of randomly generated angles as the former can give better results.}
 \textsf{[3+7+8+7 = 25 points]}

\item Here is our Google search question again. You know of the applications of tomography in medicine (CT scanning) and virology/structural biology. Your job is to search for a journal paper from any \emph{other} field which requires the use of tomographic reconstruction (examples: seismology, agriculture, gemology). State the title, venue and year of publication of the paper. State the mathematical problem defined in the paper. Take care to explain the meaning of all key terms clearly. State the method of optimization that the paper uses to solve the problem. \textsf{[16 points]}

\item Let $R_{\theta}(f)$ be the Radon transform of the image $f(x,y)$ in the direction given by $\theta$. Derive a formula for the Radon transform of the scaled image $f(ax, ay)$ where $a \neq 0$ is a scalar. \textsf{[10 points]}

\item Derive the Radon transform of the unit impulse $\delta(x,y)$ and the shifted unit impulse $\delta(x-x_0,y-y_0)$. \textsf{[10 points]}

\end{enumerate}
\end{document}